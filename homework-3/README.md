# Анализ экспериментов с глубиной полносвязных сетей

## 1. Влияние глубины на качество классификации

### 1.1. Сравнение моделей разной глубины

**Параметры эксперимента:**
- Датасет: MNIST
- Архитектуры: 1, 2, 3, 5, 7 слоёв (без регуляризации)
- Эпох: 20
- Оптимизатор: Adam, lr=1e-3
- Критерий: CrossEntropyLoss

**Результаты:**

| Глубина | Кол-во параметров | Лучшая Test Accuracy | Лучшая Train Accuracy | Время на эпоху (с) |
|---------|-------------------|---------------------|----------------------|--------------------|
| 1 слой  | 7 850             | ~0.928              | ~0.932               | ~3.6               |
| 2 слоя  | 101 770           | ~0.978              | ~0.999               | ~3.7               |
| 3 слоя  | 235 146           | ~0.981              | ~0.998               | ~3.8               |
| 5 слоёв | 575 050           | ~0.983              | ~0.997               | ~3.9               |
| 7 слоёв | 577 178           | ~0.983              | ~0.996               | ~4.0               |

**Выводы:**
- Увеличение глубины с 1 до 2 слоёв приводит к резкому росту точности на тесте (~0.93 → ~0.98).
- Дальнейшее увеличение глубины (3, 5, 7 слоёв) даёт лишь незначительный прирост точности, при этом число параметров и время обучения возрастают.
- Для MNIST оптимальной с точки зрения баланса точности и сложности является архитектура с 2-3 скрытыми слоями.

### 1.2. Анализ переобучения

**Графики обучения** (train/test accuracy по эпохам) показывают:
- Для неглубоких сетей (1-2 слоя) разница между train и test accuracy минимальна, признаки переобучения отсутствуют.
- Для более глубоких сетей (3, 5, 7 слоёв) train accuracy быстро достигает 0.99+, а test accuracy выходит на плато (~0.98), после чего наблюдается небольшое расхождение между train и test accuracy — признак переобучения.
- Чем глубже сеть, тем раньше начинается переобучение (разница между train и test accuracy появляется уже после 5-7 эпох).

## 2. Влияние регуляризации (Dropout, BatchNorm)

### Dropout

- Dropout (p=0.3) снижает train accuracy, но помогает уменьшить переобучение для глубоких сетей.
- Для неглубоких сетей эффект Dropout минимален.
- Для 5- и 7-слойных сетей Dropout позволяет удерживать test accuracy на уровне без потери обобщающей способности.

### BatchNorm

- BatchNorm ускоряет сходимость и стабилизирует обучение.
- Для всех глубин BatchNorm позволяет быстрее достичь максимальной test accuracy.
- В глубоких сетях BatchNorm уменьшает разницу между train и test accuracy, снижая переобучение.

### Dropout + BatchNorm

- Совместное применение Dropout и BatchNorm даёт наилучший баланс между скоростью сходимости, максимальной точностью и устойчивостью к переобучению.
- Для глубоких сетей (5, 7 слоёв) комбинация этих техник обеспечивает наиболее стабильное обучение.

## 3. Время обучения

- Время одной эпохи линейно растёт с увеличением числа слоёв и параметров.
- Для MNIST разница между 1- и 7-слойной сетью составляет ~0.4 секунды на эпоху.

---

## Описание графиков

- **Кривые обучения**: для неглубоких сетей train/test accuracy быстро сходятся и выходят на плато. Для глубоких сетей train accuracy продолжает расти, а test accuracy стабилизируется — появляется разрыв.
- **Влияние регуляризации**: Dropout и BatchNorm уменьшают разрыв между train и test accuracy, делают кривые обучения более гладкими и устойчивыми.

---

## Заключение

Эксперименты подтверждают, что для задачи классификации MNIST увеличение глубины сети выше 2-3 слоёв не приводит к значимому улучшению качества, но увеличивает риск переобучения. Использование регуляризационных техник (Dropout, BatchNorm) существенно повышает устойчивость глубоких моделей и рекомендуется для всех архитектур с числом слоёв больше трёх.

---

# Анализ экспериментов с шириной полносвязных сетей

## 1. Сравнение моделей разной ширины

### 1.1. Параметры эксперимента

**Параметры эксперимента:**
- Датасет: MNIST
- Архитектуры: 3 слоя с разной шириной
- Эпох: 20
- Оптимизатор: Adam, lr=1e-3
- Критерий: CrossEntropyLoss

**Конфигурации ширины:**
- Узкие слои: [64, 32, 16]
- Средние слои: [256, 128, 64]
- Широкие слои: [1024, 512, 256]
- Очень широкие слои: [2048, 1024, 512]

### 1.2. Результаты сравнения

| Конфигурация | Кол-во параметров | Лучшая Test Accuracy | Лучшая Train Accuracy | Время на эпоху (с) |
|--------------|-------------------|---------------------|----------------------|--------------------|
| Узкие        | 53 018            | 0.9744              | 0.9908               | ~3.8               |
| Средние      | 242 762           | 0.9779              | 0.9968               | ~3.8               |
| Широкие      | 1 462 538         | 0.9800              | 0.9972               | ~4.0               |
| Очень широкие| 4 235 786         | 0.9831              | 0.9976               | ~4.7               |

### 1.3. Анализ результатов

**Влияние ширины на качество:**
- Увеличение ширины слоёв приводит к монотонному росту точности на тесте.
- Переход от узких к средним слоям даёт прирост ~0.0035 (0.9744 → 0.9779).
- Переход от средних к широким слоям даёт прирост ~0.0021 (0.9779 → 0.9800).
- Переход от широких к очень широким слоям даёт прирост ~0.0031 (0.9800 → 0.9831).

**Эффективность по параметрам:**
- Узкие слои показывают наилучшую эффективность: 0.9744 точности на 53K параметров.
- Средние слои: 0.9779 точности на 243K параметров (4.6× больше параметров, +0.0035 точности).
- Широкие слои: 0.9800 точности на 1.46M параметров (27.6× больше параметров, +0.0056 точности).
- Очень широкие слои: 0.9831 точности на 4.24M параметров (80× больше параметров, +0.0087 точности).

**Время обучения:**
- Время одной эпохи растёт с увеличением числа параметров.
- Разница между узкими и очень широкими слоями составляет ~0.9 секунды на эпоху.

## 2. Grid Search по ширине

### 2.1. Методология

Проведён grid search по сетке ширины [32, 64, 128, 256, 512, 1024] для архитектуры с 3 слоями.
Схема изменения ширины: [w1, w2, w1] (расширение-сужение).

### 2.2. Ключевые результаты

**Лучшие конфигурации:**
- [1024, 512, 1024]: 0.9842 accuracy
- [1024, 256, 1024]: 0.9810 accuracy
- [256, 1024, 256]: 0.9816 accuracy
- [512, 256, 512]: 0.9811 accuracy

**Наблюдения:**
- Наилучшие результаты достигаются при использовании широких входных/выходных слоёв (512-1024) с сужением в середине.
- Схема "расширение-сужение" показывает лучшие результаты по сравнению с постоянной шириной.
- Оптимальная ширина среднего слоя находится в диапазоне 256-512 нейронов.

### 2.3. Анализ heatmap

Heatmap результатов показывает:
- Высокие значения accuracy (>0.98) сосредоточены в правом верхнем углу (широкие слои).
- Диагональные элементы (постоянная ширина) показывают средние результаты.
- Наилучшие результаты достигаются при комбинации широких внешних слоёв с умеренно широким средним слоем.

## 3. Общие выводы

### 3.1. Оптимальная архитектура

Для задачи классификации MNIST с 3-слойной архитектурой:
- **Оптимальная ширина**: [1024, 512, 1024] или [512, 256, 512]
- **Эффективность**: средние слои [256, 128, 64] обеспечивают хороший баланс точности и сложности

### 3.2. Масштабируемость

- Увеличение ширины даёт стабильный, но убывающий прирост точности.
- Эффективность по параметрам падает с ростом ширины.
- Для MNIST оптимальным является использование средних или широких слоёв с архитектурой "расширение-сужение".

---

## Описание графиков ширины

- **Кривые обучения**: все конфигурации показывают стабильную сходимость, широкие сети достигают более высокой точности, но требуют больше времени.
- **Heatmap grid search**: визуализирует зависимость точности от комбинаций ширины слоёв, показывает оптимальные области в правом верхнем углу.

---

# Анализ экспериментов с регуляризацией

## 1. Сравнение техник регуляризации

### 1.1. Параметры эксперимента

**Параметры эксперимента:**
- Датасет: MNIST
- Архитектура: 3 слоя [256, 128, 64] (одинаковая для всех экспериментов)
- Эпох: 20
- Оптимизатор: Adam, lr=1e-3
- Критерий: CrossEntropyLoss

**Исследуемые техники регуляризации:**
- Без регуляризации (baseline)
- Dropout с коэффициентами: 0.1, 0.3, 0.5
- BatchNorm
- Dropout (0.3) + BatchNorm
- L2 регуляризация (weight decay = 1e-3)
- Адаптивный Dropout (0.5 → 0.1)

### 1.2. Результаты сравнения

| Техника регуляризации | Лучшая Test Accuracy | Лучшая Train Accuracy | Разница Train-Test | Стабильность |
|----------------------|---------------------|----------------------|-------------------|--------------|
| Без регуляризации    | 0.9805              | 0.9972               | 0.0167            | Низкая       |
| Dropout 0.1          | 0.9826              | 0.9949               | 0.0123            | Средняя      |
| Dropout 0.3          | 0.9826              | 0.9872               | 0.0046            | Высокая      |
| Dropout 0.5          | 0.9809              | 0.9748               | -0.0061           | Очень высокая |
| BatchNorm            | 0.9834              | 0.9972               | 0.0138            | Средняя      |
| Dropout + BatchNorm  | 0.9852              | 0.9850               | -0.0002           | Очень высокая |
| L2 (1e-3)            | 0.9789              | 0.9869               | 0.0080            | Высокая      |
| Адаптивный Dropout   | 0.9798              | 0.9779               | -0.0019           | Очень высокая |

### 1.3. Анализ результатов

**Влияние Dropout:**
- **Dropout 0.1**: минимальное влияние на точность, небольшое улучшение стабильности
- **Dropout 0.3**: оптимальный баланс между точностью и стабильностью, лучший результат среди одиночных техник
- **Dropout 0.5**: максимальная стабильность, но снижение точности из-за избыточной регуляризации

**Влияние BatchNorm:**
- Показывает наилучшую финальную точность (0.9834) среди одиночных техник
- Ускоряет сходимость в первые эпохи
- Обеспечивает стабильное обучение, но не полностью устраняет переобучение

**Комбинированные техники:**
- **Dropout + BatchNorm**: наилучший результат (0.9852) с минимальным переобучением
- Обеспечивает наиболее стабильное обучение (разница train-test < 0.001)

**L2 регуляризация:**
- Показывает умеренные результаты (0.9789)
- Менее эффективна по сравнению с Dropout и BatchNorm

**Адаптивный Dropout:**
- Показывает хорошую стабильность (0.9798)
- Менее эффективен по сравнению с фиксированным Dropout 0.3

## 2. Анализ стабильности обучения

### 2.1. Критерии стабильности

**Метрики стабильности:**
- Разница между train и test accuracy
- Стандартное отклонение test accuracy по эпохам
- Время до сходимости

### 2.2. Ранжирование по стабильности

1. **Dropout + BatchNorm** (наилучшая стабильность)
2. **Адаптивный Dropout**
3. **Dropout 0.5**
4. **L2 регуляризация**
5. **Dropout 0.3**
6. **BatchNorm**
7. **Dropout 0.1**
8. **Без регуляризации** (наименьшая стабильность)

## 3. Анализ распределения весов

### 3.1. Влияние регуляризации на веса

**Без регуляризации:**
- Широкое распределение весов
- Наличие экстремальных значений
- Нестабильная структура

**Dropout:**
- Более компактное распределение весов
- Уменьшение экстремальных значений
- Стабилизация с ростом коэффициента

**BatchNorm:**
- Нормализованное распределение весов
- Стабильная структура
- Улучшенная сходимость

**L2 регуляризация:**
- Сжатие распределения весов к нулю
- Уменьшение дисперсии
- Более равномерное распределение


## Общие выводы

1. **Комбинированные техники** превосходят одиночные
2. **Dropout 0.3** является оптимальным для большинства случаев
3. **BatchNorm** эффективен для ускорения сходимости
4. **L2 регуляризация** менее эффективна на MNIST

---

## Описание графиков регуляризации

- **Кривые обучения**: Dropout + BatchNorm показывает наиболее стабильные кривые с минимальным разрывом между train и test accuracy
- **Распределения весов**: регуляризация приводит к более компактным и стабильным распределениям

